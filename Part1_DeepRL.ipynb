{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "# Part 1: Deep Reinforcement Learning\n",
    "<a id=part1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial we have seen value-based reinforcement learning, in which we learn to approximate the action-value function $q(s,a)$.\n",
    "\n",
    "In this exercise we'll explore a different approach, directly learning the agent's policy distribution, $\\pi(a|s)$\n",
    "by using *policy gradients*, in order to safely land on the moon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prefer CPU, GPU won't help much in this assignment\n",
    "device = 'cpu'\n",
    "print('Using device:', device)\n",
    "\n",
    "# Seed for deterministic tests\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some technical notes before we begin:\n",
    "\n",
    "- This part does not require a GPU. We won't need large models, and the computation bottleneck will be the generation of episodes to train on.\n",
    "- In order to run this notebook on the server, you must prepend the `xvfb-run` command to create a virtual screen. For example,\n",
    "    - to run this notebook with `srun` do\n",
    "        ```\n",
    "        srun -c2 --gres=gpu:1 xvfb-run -a -s \"-screen 0 1440x900x24\" python main.py run-nb <filename>\n",
    "        ```\n",
    "    - To run the submission script, do\n",
    "        ```\n",
    "        srun -c2 xvfb-run -a -s \"-screen 0 1440x900x24\" python main.py prepare-submission ...\n",
    "        ```\n",
    "    - note that we have already included the `xvfb-run` command inside the `jupyter-lab.sh` script, so you can use it as usual with `srun`.\n",
    "    and so on.\n",
    "- The OpenAI `gym` library is not officially supported on windows. However it should be possible to install and run the necessary environment for this exercise. However, we cannot provide you with technical support for this. If you have trouble installing locally, we suggest running on the course server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradients\n",
    "<a id=part1_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the tutorial that we define the **policy** of an agent as the conditional distribution,\n",
    "$$\n",
    "\\pi(a|s) = \\Pr(a_t=a\\vert s_t=s),\n",
    "$$\n",
    "which defines how likely the agent is to take action $a$ at state $s$.\n",
    "\n",
    "Furthermore we define the action-value function,\n",
    "$$\n",
    "q_{\\pi}(s,a) = \\E{g_t(\\tau)|s_t = s,a_t=a,\\pi}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "g_t(\\tau) = r_{t+1}+\\gamma r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+1+k},\n",
    "$$\n",
    "is the total discounted reward of a specific trajectory $\\tau$ from time $t$, and the expectation in $q$ is over all possible\n",
    "trajectories,\n",
    "$\n",
    "\\tau=\\left\\{ (s_0,a_0,r_1,s_1), \\dots (s_T,a_T,r_{T+1},s_{T+1}) \\right\\}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial we saw that we can learn a value function starting with some random function and\n",
    "updating it iteratively by using the **Bellman optimality equation**.\n",
    "Given that we have some action-value function, we can immediately create a policy based on that\n",
    "by simply selecting an action which maximize the action-value at the current state, i.e.\n",
    "$$\n",
    "\\pi(a|s) =\n",
    "\\begin{cases}\n",
    "1, & a = \\arg\\max_{a'\\in\\cset{A}} q(s,a') \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}.\n",
    "$$\n",
    "This is called $q$-learning. This approach aims to obtain a policy indirectly through the action-value function.\n",
    "Yet, in most cases we don't actually care about knowing the value of particular states,\n",
    "since all we need is a good policy for our agent. \n",
    "\n",
    "Here we'll take a different approach and learn a policy distribution $\\pi(a|s)$ directly - by using **policy gradients**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a parametric policy, $\\pi_\\vec{\\theta}(a|s)$, and maximize total discounted reward (or minimize the negative reward):\n",
    "$$\n",
    "\\mathcal{L}(\\vec{\\theta})=\\E[\\tau]{-g(\\tau)|\\pi_\\vec{\\theta}} = -\\int g(\\tau)p(\\tau|\\vec{\\theta})d\\tau,\n",
    "$$\n",
    "where $p(\\tau|\\vec{\\theta})$ is the probability of a specific trajectory $\\tau$ under the policy defined by $\\vec{\\theta}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to find the parameters $\\vec{\\theta}$ which minimize $\\mathcal{L}(\\vec{\\theta})$, we'll compute the gradient w.r.t. $\\vec{\\theta}$:\n",
    "$$\n",
    "\\grad\\mathcal{L}(\\vec{\\theta}) = -\\int g(\\tau)\\grad p(\\tau|\\vec{\\theta})d\\tau.\n",
    "$$\n",
    "\n",
    "Unfortunately, if we try to write $p(\\tau|\\vec{\\theta})$ explicitly,\n",
    "we find that computing it's gradient with respect to $\\vec{\\theta}$ is\n",
    "quite intractable due to a huge product of terms depending on $\\vec{\\theta}$:\n",
    "$$\n",
    "p(\\tau|\\vec{\\theta})=p\\left(\\left\\{ (s_t,a_t,r_{t+1},s_{t+1})\\right\\}_{t\\geq0}\\given\\vec{\\theta}\\right)\n",
    "=p(s_0)\\prod_{t\\geq0} \\pi_{\\vec{\\theta}}(a_t|s_t)p(s_{t+1}|s_t,a_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by using the fact that $\\grad_{x}\\log(f(x))=\\frac{\\grad_{x}f(x)}{f(x)}$, we can convert the product into a sum:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\grad\\mathcal{L}(\\vec{\\theta})\n",
    "&= -\\int g(\\tau)\\grad p(\\tau|\\vec{\\theta})d\\tau\n",
    "= -\\int g(\\tau)\\frac{\\grad p(\\tau|\\vec{\\theta})}{p(\\tau|\\vec{\\theta})}p(\\tau|\\vec{\\theta})d\\tau \\\\\n",
    "&= -\\int g(\\tau)\\grad\\log\\left(p(\\tau|\\vec{\\theta})\\right)p(\\tau|\\vec{\\theta})d\\tau \\\\\n",
    "&= -\\int g(\\tau)\\grad\\log\\left( p(s_0)\\prod_{t\\geq0} \\pi_{\\vec{\\theta}}(a_t|s_t)p(s_{t+1}|s_t,a_t) \\right)\n",
    "p(\\tau|\\vec{\\theta})d\\tau \\\\\n",
    "&= -\\int g(\\tau)\\grad\\left( \\log p(s_0) + \\sum_{t\\geq0} \\log \\pi_{\\vec{\\theta}}(a_t|s_t) + \n",
    "\\sum_{t\\geq0}\\log p(s_{t+1}|s_t,a_t) \\right) p(\\tau|\\vec{\\theta})d\\tau \\\\\n",
    "&= -\\int g(\\tau)\\sum_{t\\geq0} \\grad\\log \\pi_{\\vec{\\theta}}(a_t|s_t) p(\\tau|\\vec{\\theta})d\\tau \\\\\n",
    "&= \\E[\\tau]{-g(\\tau)\\sum_{t\\geq0} \\grad\\log \\pi_{\\vec{\\theta}}(a_t|s_t)}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"vanilla\" version of the policy gradient. We can interpret is as a weighted log-likelihood function.\n",
    "The log-policy is the log-likelihood term we wish to maximize and the total discounted reward acts as a weight: high-return positive\n",
    "trajectories will cause the probability of actions taken during them to increase, and negative-return trajectories will cause the\n",
    "probabilities of actions taken to decrease.\n",
    "\n",
    "In the following figures we see three trajectories: high-return positive-reward (green), low-return positive-reward (yellow) and negative-return (red) and the action probabilities along the trajectories after the update. Credit: Sergey Levine.\n",
    "\n",
    "|<strong></strong>||\n",
    "|-----| ----|\n",
    "|<img src=\"imgs/pg1.png\" height=\"200\">|<img src=\"imgs/pg2.png\" height=\"200\">|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major drawback of the policy-gradient is it's high variance, which causes erratic optimization behavior and therefore slow convergence.\n",
    "One reason for this is that the log-policy weight term, $g(\\tau)$ can vary wildly between different trajectories, even if they're similar in\n",
    "actions. Later on we'll implement the loss and explore some methods of variance reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landing on the moon with policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spirit of the recent achievements of the Israeli space industry,\n",
    "we'll apply our reinforcement learning skills to solve a simple game called **LunarLander**.\n",
    "\n",
    "This game is available as an `environment` in OpenAI `gym`.\n",
    "\n",
    "<video loop autoplay src=\"http://gym.openai.com/videos/2019-04-06--My9IiAbqha/LunarLander-v2/original.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, you need to control the lander and get it to land safely on the moon.\n",
    "To do so, you must apply bottom, right or left thrusters (each are either fully on or fully off)\n",
    "and get it to land within the designated zone as quickly as possible and with minimal wasted fuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Just for fun :) ... but also to re-define the default max number of steps\n",
    "ENV_NAME = 'Beresheet-v2'\n",
    "MAX_EPISODE_STEPS = 300\n",
    "if ENV_NAME not in gym.envs.registry.env_specs:\n",
    "    gym.register(\n",
    "        id=ENV_NAME,\n",
    "        entry_point='gym.envs.box2d:LunarLander',\n",
    "        max_episode_steps=MAX_EPISODE_STEPS,\n",
    "        reward_threshold=200,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<LunarLander<Beresheet-v2>>>\n",
      "observations space: Box(-inf, inf, (8,), float32)\n",
      "action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "print(env)\n",
    "print(f'observations space: {env.observation_space}')\n",
    "print(f'action space: {env.action_space}')\n",
    "\n",
    "ENV_N_ACTIONS = env.action_space.n\n",
    "ENV_N_OBSERVATIONS = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations at each step is the Lander's position, velocity, angle, angular velocity and ground contact state.\n",
    "The actions are no-op, fire left truster, bottom thruster and right thruster.\n",
    "\n",
    "You are **highly encouraged** to read the [documentation](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py) in the source code of the `LunarLander` environment to understand the reward system,\n",
    "and see how the actions and observations are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy network and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with our policy-model. This will be a simple neural net, which should take an observation and return a score for each possible action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Implement all methods in the `PolicyNet` class in the `hw4/rl_pg.py` module.\n",
    "   Start small. A simple MLP with a few hidden layers is a good starting point. You can come back and change it later based on the the experiments.  \n",
    "   Notice that we'll use the `build_for_env` method to instantiate a `PolicyNet` based on the configuration of a given environment.\n",
    "2. If you need hyperparameters to configure your model (e.g. number of hidden layers, sizes, etc.), add them in `part1_pg_hyperparams()` in `hw4/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(8, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (3): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hw4.rl_pg as hw4pg\n",
    "import hw4.answers\n",
    "\n",
    "hp = hw4.answers.part1_pg_hyperparams()\n",
    "\n",
    "# You can add keyword-args to this function which will be populated from the\n",
    "# hyperparameters dict.\n",
    "p_net = hw4pg.PolicyNet.build_for_env(env, device, **hp)\n",
    "p_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need an **agent**. The purpose of our agent will be to act according to the current policy and generate experiences.\n",
    "Our `PolicyAgent` will use a `PolicyNet` as the current policy function.\n",
    "\n",
    "\n",
    "We'll also define some extra datatypes to help us represent the data generated by our agent.\n",
    "You can find the `Experience`, `Episode` and `TrainBatch` datatypes in the `hw4/rl_data.py` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `current_action_distribution()` method of the `PolicyAgent` class in the `hw4/rl_pg.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0018,  1.4145,  0.1773,  0.1578, -0.0020, -0.0402,  0.0000,  0.0000])\n",
      "tensor([1., 1., 1., 1.])\n",
      "torch.Size([4]) (4,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor(4.) != 1.0 within 1e-05 delta (tensor(3.) difference)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-261-c0d5e9f07757>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massertSequenceEqual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massertAlmostEqual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs236781-hw\\lib\\unittest\\case.py\u001b[0m in \u001b[0;36massertAlmostEqual\u001b[1;34m(self, first, second, places, msg, delta)\u001b[0m\n\u001b[0;32m    964\u001b[0m                 safe_repr(diff))\n\u001b[0;32m    965\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     def assertNotAlmostEqual(self, first, second, places=None, msg=None,\n",
      "\u001b[1;31mAssertionError\u001b[0m: tensor(4.) != 1.0 within 1e-05 delta (tensor(3.) difference)"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    agent = hw4pg.PolicyAgent(env, p_net, device)\n",
    "    d = agent.current_action_distribution()\n",
    "    print(d.shape, (env.action_space.n,))\n",
    "    \n",
    "    test.assertSequenceEqual(d.shape, (env.action_space.n,))\n",
    "    test.assertAlmostEqual(d.sum(), 1.0, delta=1e-5)\n",
    "    \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `step()` method of the `PolicyAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience(state=tensor([-0.0108,  1.4117, -0.5376,  0.0037,  0.0102,  0.0785,  0.0000,  0.0000]), action=3, reward=0.8279251108782273, is_done=False)\n"
     ]
    }
   ],
   "source": [
    "agent = hw4pg.PolicyAgent(env, p_net, device)\n",
    "exp = agent.step()\n",
    "\n",
    "test.assertIsInstance(exp, hw4pg.Experience)\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our agent, we'll write some code that allows it to play an environment. We'll use the `Monitor`\n",
    "wrapper in `gym` to generate a video of the episode for visual debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `monitor_episode()` method of the `PolicyAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, n_steps, reward = agent.monitor_episode(ENV_NAME, p_net, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the Monitor video in this notebook, we'll use a helper function from our `jupyter_utils` and a small wrapper that extracts the path of the last video file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Monitor<TimeLimit<LunarLander<Beresheet-v2>>>> 87 -347.7887420579931\n"
     ]
    }
   ],
   "source": [
    "import cs236781.jupyter_utils as jupyter_utils\n",
    "\n",
    "def show_monitor_video(monitor_env, idx=0, **kw):\n",
    "    # Extract video path\n",
    "    video_path = monitor_env.videos[idx][0]\n",
    "    video_path = os.path.relpath(video_path, start=os.path.curdir)\n",
    "    \n",
    "    # Use helper function to embed the video\n",
    "    return jupyter_utils.show_video_in_notebook(video_path, **kw)\n",
    "print(env, n_steps, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ran for 87 steps. Total reward: -347.79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"data:video/mp4;base64,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\" controls autoplay width=\"500\" height=\"auto\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Episode ran for {n_steps} steps. Total reward: {reward:.2f}')\n",
    "\n",
    "show_monitor_video(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create data to train on.\n",
    "We need to train on batches of state-action pairs, so that our network can learn to predict the actions.\n",
    "\n",
    "We'll split this task into three parts:\n",
    "1. Generate a batch of `Episode`s, by using an `Agent` that's playing according to our current policy network.\n",
    "   Each `Episode` object contains the `Experience` objects created by the agent.\n",
    "2. Calculate the total discounted reward for each state we encountered and action we took. This is our action-value estimate.\n",
    "3. Convert the `Episode`s into a batch of tensors to train on.\n",
    "   Each batch will contain states, action taken per state, reward accrued, and the calculated estimated state-values.\n",
    "   These will be stored in a `TrainBatch` object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `episode_batch_generator()` method in the `TrainBatchDataset` class within the `hw4.rl_data` module. This will address part 1 in the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object TrainBatchDataset.episode_batch_generator at 0x00000141BE78EB30>\n",
      "First episode: Episode(total_reward=-1634.81, #experences=58)\n"
     ]
    }
   ],
   "source": [
    "import hw4.rl_data as hw4data\n",
    "\n",
    "def agent_fn():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    hp = hw4.answers.part1_pg_hyperparams()\n",
    "    p_net = hw4pg.PolicyNet.build_for_env(env, device, **hp)\n",
    "    return hw4pg.PolicyAgent(env, p_net, device)\n",
    "    \n",
    "ds = hw4data.TrainBatchDataset(agent_fn, episode_batch_size=8, gamma=0.9)\n",
    "batch_gen = ds.episode_batch_generator()\n",
    "print(batch_gen)\n",
    "b = next(batch_gen)\n",
    "print('First episode:', b[0])\n",
    "\n",
    "test.assertEqual(len(b), 8)\n",
    "for ep in b:\n",
    "    test.assertIsInstance(ep, hw4data.Episode)\n",
    "    \n",
    "    # Check that it's a full episode\n",
    "    is_done = [exp.is_done for exp in ep.experiences]\n",
    "    test.assertFalse(any(is_done[0:-1]))\n",
    "    test.assertTrue(is_done[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `calc_qvals()` method in the `Episode` class.\n",
    "This will address part 2.\n",
    "These q-values are an estimate of the actual action value function: $$\\hat{q}_{t} = \\sum_{t'\\geq t} \\gamma^{t'-t}r_{t'+1}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "test_rewards = np.random.randint(-10, 10, 100)\n",
    "test_experiences = [hw4pg.Experience(None,None,r,False) for r in test_rewards] \n",
    "test_episode = hw4data.Episode(np.sum(test_rewards), test_experiences)\n",
    "\n",
    "qvals = test_episode.calc_qvals(0.9)\n",
    "qvals = list(qvals)\n",
    "\n",
    "expected_qvals = np.load(os.path.join('tests', 'assets', 'part1_expected_qvals.npy'))\n",
    "for i in range(len(test_rewards)):\n",
    "    test.assertAlmostEqual(expected_qvals[i], qvals[i], delta=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `from_episodes()` method in the `TrainBatch` class.\n",
    "This will address part 3.\n",
    "\n",
    "Notes:\n",
    "- The `TrainBatchDataset` class provides a generator function that will use the above function to lazily generate batches of training samples and labels on demand.\n",
    "- This allows us to use a standard `PyTorch` dataloader to wrap our Dataset and provide us with parallel data loading for free!\n",
    "  This means we can run multiple environments with multiple agents in separate background processes to generate data for training and thus prevent the data loading bottleneck which is caused by the fact that we must generate full Episodes to train on in order to calculate the q-values.\n",
    "- We'll set the `DataLoader`'s `batch_size` to `None` because we have already implemented custom batching in our dataset.\n",
    "- You can choose the number of worker processes generating data using the `num_workers` parameter in the hyperparams dict. Set `num_workers=0` to disable parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: TrainBatch(states: torch.Size([613, 8]), actions: torch.Size([613]), q_vals: torch.Size([613])), num_episodes: 613)\n",
      "#1: TrainBatch(states: torch.Size([608, 8]), actions: torch.Size([608]), q_vals: torch.Size([608])), num_episodes: 608)\n",
      "#2: TrainBatch(states: torch.Size([541, 8]), actions: torch.Size([541]), q_vals: torch.Size([541])), num_episodes: 541)\n",
      "#3: TrainBatch(states: torch.Size([542, 8]), actions: torch.Size([542]), q_vals: torch.Size([542])), num_episodes: 542)\n",
      "#4: TrainBatch(states: torch.Size([526, 8]), actions: torch.Size([526]), q_vals: torch.Size([526])), num_episodes: 526)\n",
      "#5: TrainBatch(states: torch.Size([543, 8]), actions: torch.Size([543]), q_vals: torch.Size([543])), num_episodes: 543)\n",
      "#6: TrainBatch(states: torch.Size([597, 8]), actions: torch.Size([597]), q_vals: torch.Size([597])), num_episodes: 597)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hp = hw4.answers.part1_pg_hyperparams()\n",
    "\n",
    "ds = hw4data.TrainBatchDataset(agent_fn, episode_batch_size=8, gamma=0.9)\n",
    "dl = DataLoader(\n",
    "    ds,\n",
    "    batch_size=None,\n",
    "    num_workers=hp['num_workers'],\n",
    "    multiprocessing_context='fork' if hp['num_workers'] > 0 else None\n",
    ")\n",
    "\n",
    "\n",
    "for i, train_batch in enumerate(dl):\n",
    "    states, actions, qvals, reward_mean = train_batch\n",
    "    print(f'#{i}: {train_batch}')\n",
    "    test.assertEqual(states.shape[0], actions.shape[0])\n",
    "    test.assertEqual(qvals.shape[0], actions.shape[0])\n",
    "    test.assertEqual(states.shape[1], env.observation_space.shape[0])\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we need a loss function to optimize over.\n",
    "We'll calculate three types of losses:\n",
    "1. The causal vanilla policy gradient loss.\n",
    "1. The policy gradient loss, with a baseline to reduce variance.\n",
    "2. An entropy-based loss whos purpose is to diversify the agent's action selection,\n",
    "   and prevent it from being \"too sure\" about its actions.\n",
    "   This loss will be used together with one of the above losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal vanilla policy-gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have derived the policy-gradient as\n",
    "$$\n",
    "\\grad\\mathcal{L}(\\vec{\\theta}) = \\E[\\tau]{-g(\\tau)\\sum_{t\\geq0} \\grad\\log \\pi_{\\vec{\\theta}}(a_t|s_t)}.\n",
    "$$\n",
    "\n",
    "By writing the discounted reward explicitly and enforcing causality, i.e. the action taken at time $t$ can't affect\n",
    "the reward at time $t'<t$, we can get a slightly lower-variance version of the policy gradient:\n",
    "\n",
    "$$\n",
    "\\grad\\mathcal{L}_{\\text{PG}}(\\vec{\\theta}) = \n",
    "\\E[\\tau]{-\\sum_{t\\geq0} \\left(\\sum_{t'\\geq t} \\gamma^{t'}r_{t'+1} \\right)\\grad\\log \\pi_{\\vec{\\theta}}(a_t|s_t)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the expectation over trajectories is calculated using a Monte-Carlo approach, i.e. simply sampling $N$\n",
    "trajectories and average the term inside the expectation. Therefore, we will use the following estimated version of the policy gradient:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat\\grad\\mathcal{L}_{\\text{PG}}(\\vec{\\theta})\n",
    "&=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t\\geq0} \\left(\\sum_{t'\\geq t} \\gamma^{t'}r_{i,t'+1} \\right)\\grad\\log \\pi_{\\vec{\\theta}}(a_{i,t}|s_{i,t}) \\\\\n",
    "&=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t\\geq0} \\hat{q}_{i,t} \\grad\\log \\pi_{\\vec{\\theta}}(a_{i,t}|s_{i,t}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note the use of the notation $\\hat{q}_{i,t}$ to represent the estimated action-value at time $t$ in the sampled trajectory $i$.\n",
    "Here $\\hat{q}_{i,t}$ is acting as the weight-term for the policy gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `VanillaPolicyGradientLoss` class in the `hw4/rl_pg.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_batch=TrainBatch(states: torch.Size([290, 8]), actions: torch.Size([290]), q_vals: torch.Size([290])), num_episodes: 290)\n",
      "\n",
      "test_action_scores=\n",
      "tensor([[ 0.3189, -0.4245,  0.3057, -0.7746],\n",
      "        [ 0.0349,  0.3211,  1.5736, -0.8455],\n",
      "        [-1.2742,  2.1228, -1.2347, -0.4879],\n",
      "        ...,\n",
      "        [ 0.5585,  0.3491,  0.8484,  2.0355],\n",
      "        [ 0.6575,  0.6645, -1.0752,  0.1760],\n",
      "        [-0.5064, -0.8442, -0.2214,  2.2746]])\n",
      "shape=torch.Size([290, 4])\n",
      "\n",
      "loss_p=tensor(38.3144)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "38.314414978027344 != -36.642 within 0.01 delta (74.95641497802734 difference)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-237-522a8e3585c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{loss_p=}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massertAlmostEqual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m36.642\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\cs236781-hw\\lib\\unittest\\case.py\u001b[0m in \u001b[0;36massertAlmostEqual\u001b[1;34m(self, first, second, places, msg, delta)\u001b[0m\n\u001b[0;32m    964\u001b[0m                 safe_repr(diff))\n\u001b[0;32m    965\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     def assertNotAlmostEqual(self, first, second, places=None, msg=None,\n",
      "\u001b[1;31mAssertionError\u001b[0m: 38.314414978027344 != -36.642 within 0.01 delta (74.95641497802734 difference)"
     ]
    }
   ],
   "source": [
    "# Ensure deterministic run\n",
    "env = gym.make(ENV_NAME)\n",
    "env.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def agent_fn():\n",
    "    # Use a simple \"network\" here, so that this test doesn't depend on\n",
    "    # your specific PolicyNet implementation\n",
    "    p_net_test = nn.Linear(ENV_N_OBSERVATIONS, ENV_N_ACTIONS, bias=True)\n",
    "    agent = hw4pg.PolicyAgent(env, p_net_test)\n",
    "    return agent\n",
    "\n",
    "dataloader = hw4data.TrainBatchDataset(agent_fn, gamma=0.9, episode_batch_size=4)\n",
    "\n",
    "test_batch = next(iter(dataloader))\n",
    "test_action_scores = torch.randn(len(test_batch), env.action_space.n)\n",
    "print(f\"{test_batch=}\", end='\\n\\n')\n",
    "print(f\"test_action_scores=\\n{test_action_scores}\\nshape={test_action_scores.shape}\", end='\\n\\n')\n",
    "\n",
    "loss_fn_p = hw4pg.VanillaPolicyGradientLoss()\n",
    "loss_p, _ = loss_fn_p(test_batch, test_action_scores)\n",
    "\n",
    "print(f'{loss_p=}')\n",
    "test.assertAlmostEqual(loss_p.item(), -36.642, delta=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy-gradient with baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to reduce the variance of our gradient is to use relative weighting of the log-policy instead of absolute reward values.\n",
    "$$\n",
    "\\hat\\grad\\mathcal{L}_{\\text{BPG}}(\\vec{\\theta})\n",
    "=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t\\geq0} \\left(\\hat{q}_{i,t}-b\\right) \\grad\\log \\pi_{\\vec{\\theta}}(a_{i,t}|s_{i,t}).\n",
    "$$\n",
    "In other words, we don't measure a trajectory's worth by it's total reward, but by how much better that total reward is relative to some\n",
    "expected (\"baseline\") reward value, denoted above by $b$.\n",
    "Note that subtracting a baseline has no effect on the expected value of the policy gradient. It's easy to prove this directly by definition.\n",
    "\n",
    "Here we'll implement a very simple baseline (not optimal in terms of variance reduction): the average of the estimated state-values $\\hat{q}_{i,t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `BaselinePolicyGradientLoss` class in the `hw4/rl_pg.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same batch and action_scores from above cell\n",
    "loss_fn_p = hw4pg.BaselinePolicyGradientLoss()\n",
    "loss_p, loss_dict = loss_fn_p(test_batch, test_action_scores)\n",
    "\n",
    "print(f'{loss_dict=}')\n",
    "test.assertAlmostEqual(loss_dict['baseline'], -22.191, delta=1e-2)\n",
    "test.assertAlmostEqual(loss_p.item(), -0.278, delta=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a probability distribution (in our case the policy), is\n",
    "$$\n",
    "H(\\pi) = -\\sum_{a} \\pi(a|s)\\log\\pi(a|s).\n",
    "$$\n",
    "The entropy is always positive and obtains it's maximum for a uniform distribution.\n",
    "We'll use the entropy of the policy as a bonus, i.e. we'll try to maximize it.\n",
    "The idea is the prevent the policy distribution from becoming too narrow and thus promote the agent's exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll calculate the maximal possible entropy value of the action distribution for a set number of possible actions.\n",
    "This will be used as a normalization term.\n",
    "\n",
    "**TODO**: Complete the implementation of the `calc_max_entropy()` method in the `ActionEntropyLoss` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:17:47.342049Z",
     "iopub.status.busy": "2021-01-26T09:17:47.341565Z",
     "iopub.status.idle": "2021-01-26T09:17:47.362435Z",
     "shell.execute_reply": "2021-01-26T09:17:47.362999Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn_e = hw4pg.ActionEntropyLoss(env.action_space.n)\n",
    "print('max_entropy = ', loss_fn_e.max_entropy)\n",
    "\n",
    "test.assertAlmostEqual(loss_fn_e.max_entropy, 1.38629436, delta=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `forward()` method in the `ActionEntropyLoss` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:17:47.366171Z",
     "iopub.status.busy": "2021-01-26T09:17:47.365684Z",
     "iopub.status.idle": "2021-01-26T09:17:47.387917Z",
     "shell.execute_reply": "2021-01-26T09:17:47.388478Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_e, _ = loss_fn_e(test_batch, test_action_scores)\n",
    "print('loss = ', loss_e)\n",
    "\n",
    "test.assertAlmostEqual(loss_e.item(), -0.8103, delta=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement our training procedure as follows:\n",
    "\n",
    "1. Initialize the current policy to be a random policy.\n",
    "1. Sample $N$ trajectories from the environment using the current policy.\n",
    "2. Calculate the estimated $q$-values, $\\hat{q}_{i,t} = \\sum_{t'\\geq t} \\gamma^{t'}r_{i,t'+1}$ for each trajectory $i$.\n",
    "3. Calculate policy gradient estimate $\\hat\\grad\\mathcal{L}(\\vec{\\theta})$ as defined above.\n",
    "4. Perform SGD update $\\vec{\\theta}\\leftarrow\\vec{\\theta}-\\eta\\hat\\grad\\mathcal{L}(\\vec{\\theta})$.\n",
    "5. Repeat from step 2.\n",
    "\n",
    "This is known as the **REINFORCE** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we've already implemented everything we need for steps 1-4 so we need only a bit more code to put it all together.\n",
    "\n",
    "The following block implements a wrapper, `train_pg` to create all the objects we need in order to train our policy gradient model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:17:47.396286Z",
     "iopub.status.busy": "2021-01-26T09:17:47.395771Z",
     "iopub.status.idle": "2021-01-26T09:17:47.418234Z",
     "shell.execute_reply": "2021-01-26T09:17:47.418597Z"
    }
   },
   "outputs": [],
   "source": [
    "import hw4.answers\n",
    "from functools import partial\n",
    "\n",
    "ENV_NAME = \"Beresheet-v2\"\n",
    "\n",
    "def agent_fn_train(agent_type, p_net, seed, envs_dict):\n",
    "    winfo = torch.utils.data.get_worker_info()\n",
    "    wid = winfo.id if winfo else 0\n",
    "    seed = seed + wid if seed else wid\n",
    "\n",
    "    env = gym.make(ENV_NAME)\n",
    "    envs_dict[wid] = env\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    return agent_type(env, p_net)\n",
    "\n",
    "def train_rl(agent_type, net_type, loss_fns, hp, seed=None, checkpoints_file=None, **train_kw):\n",
    "    print(f'hyperparams: {hp}')\n",
    "    \n",
    "    envs = {}\n",
    "    p_net = net_type(ENV_N_OBSERVATIONS, ENV_N_ACTIONS, **hp)\n",
    "    p_net.share_memory()\n",
    "    agent_fn = partial(agent_fn_train, agent_type, p_net, seed, envs)\n",
    "    \n",
    "    dataset = hw4data.TrainBatchDataset(agent_fn, hp['batch_size'], hp['gamma'])\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=None, num_workers=hp['num_workers'],\n",
    "        multiprocessing_context='fork' if hp['num_workers'] > 0 else None\n",
    "    )\n",
    "    optimizer = optim.Adam(p_net.parameters(), lr=hp['learn_rate'], eps=hp['eps'])\n",
    "    \n",
    "    trainer = hw4pg.PolicyTrainer(p_net, optimizer, loss_fns, dataloader, checkpoints_file)\n",
    "    try:\n",
    "        trainer.train(**train_kw)\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Training interrupted by user.')\n",
    "    finally:\n",
    "        for env in envs.values():\n",
    "            env.close()\n",
    "\n",
    "    # Include final model state\n",
    "    training_data = trainer.training_data\n",
    "    training_data['model_state'] = p_net.state_dict()\n",
    "    return training_data\n",
    "    \n",
    "def train_pg(baseline=False, entropy=False, **train_kwargs):\n",
    "    hp = hw4.answers.part1_pg_hyperparams()\n",
    "    \n",
    "    loss_fns = []\n",
    "    if baseline:\n",
    "        loss_fns.append(hw4pg.BaselinePolicyGradientLoss())\n",
    "    else:\n",
    "        loss_fns.append(hw4pg.VanillaPolicyGradientLoss())\n",
    "    if entropy:\n",
    "        loss_fns.append(hw4pg.ActionEntropyLoss(ENV_N_ACTIONS, hp['beta']))\n",
    "\n",
    "    return train_rl(hw4pg.PolicyAgent, hw4pg.PolicyNet, loss_fns, hp, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PolicyTrainer` class implements the training loop, collects the losses and rewards and provides some useful checkpointing functionality.\n",
    "The training loop will generate batches of episodes and train on them until either:\n",
    "- The average total reward from the last `running_mean_len` episodes is greater than the `target_reward`, OR\n",
    "- The number of generated episodes reached `max_episodes`.\n",
    "\n",
    "Most of this class is already implemented for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Complete the training loop by implementing the `train_batch()` method of the `PolicyTrainer`.\n",
    "2. Tweak the hyperparameters in the `part1_pg_hyperparams()` function within the `hw4/answers.py` module as needed. You get some sane defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether our model is actually training.\n",
    "We'll try to reach a very low (bad) target reward, just as a sanity check to see that training works.\n",
    "Your model should be able to reach this target reward within a few batches.\n",
    "\n",
    "You can increase the target reward and use this block to manually tweak your model and hyperparameters a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:17:47.422376Z",
     "iopub.status.busy": "2021-01-26T09:17:47.421884Z",
     "iopub.status.idle": "2021-01-26T09:18:02.404013Z",
     "shell.execute_reply": "2021-01-26T09:18:02.404808Z"
    }
   },
   "outputs": [],
   "source": [
    "target_reward = -140 # VERY LOW target\n",
    "train_data = train_pg(target_reward=target_reward, seed=SEED, max_episodes=2000, running_mean_len=10)\n",
    "\n",
    "test.assertGreater(train_data['mean_reward'][-1], target_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with different losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now run a few experiments to see the effect of diferent loss functions on the training dynamics. Namely, we'll try:\n",
    "1. Vanilla PG (`vpg`): No baseline, no entropy\n",
    "2. Baseline PG (`bpg`): Baseline, no entropy loss\n",
    "3. Entropy PG (`epg`): No baseline, with entropy loss\n",
    "3. Combined PG (`cpg`): Baseline, with entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:02.409353Z",
     "iopub.status.busy": "2021-01-26T09:18:02.408844Z",
     "iopub.status.idle": "2021-01-26T09:18:02.432680Z",
     "shell.execute_reply": "2021-01-26T09:18:02.433225Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from pprint import pprint\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "ExpConfig = namedtuple('ExpConfig', ('name','baseline','entropy'))\n",
    "\n",
    "def exp_configs():\n",
    "    exp_names = ('vpg', 'epg', 'bpg', 'cpg')\n",
    "    z = zip(exp_names, it.product((False, True), (False, True)))\n",
    "    return (ExpConfig(n, b, e) for (n, (b, e)) in z)\n",
    "\n",
    "pprint(list(exp_configs()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the training data from each experiment for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:02.436825Z",
     "iopub.status.busy": "2021-01-26T09:18:02.436330Z",
     "iopub.status.idle": "2021-01-26T09:18:02.457748Z",
     "shell.execute_reply": "2021-01-26T09:18:02.458103Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def dump_training_data(data, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, mode='wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "        \n",
    "def load_training_data(filename):\n",
    "    with open(filename, mode='rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the experiments! We'll run each configuration for a fixed number of episodes so that we can compare them.\n",
    "\n",
    "Notes:\n",
    "1. Until your models start working, you can decrease the number of episodes for each experiment, or only run one experiment.\n",
    "2.  The results will be saved in a file. To re-run the experiments, you can set `force_run` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:02.462232Z",
     "iopub.status.busy": "2021-01-26T09:18:02.461744Z",
     "iopub.status.idle": "2021-01-26T09:18:02.499868Z",
     "shell.execute_reply": "2021-01-26T09:18:02.500465Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "exp_max_episodes = 4000\n",
    "\n",
    "results = {}\n",
    "training_data_filename = os.path.join('results', f'part1_exp.dat')\n",
    "\n",
    "# Set to True to force re-run (careful! will delete old experiment results)\n",
    "force_run = False\n",
    "\n",
    "# Skip running if results file exists.\n",
    "if os.path.isfile(training_data_filename) and not force_run:\n",
    "    print(f'=== results file {training_data_filename} exists, skipping experiments.')\n",
    "    results = load_training_data(training_data_filename)\n",
    "    \n",
    "else:\n",
    "    for n, b, e in exp_configs():\n",
    "        print(f'=== Experiment {n}')\n",
    "        results[n] = train_pg(baseline=b, entropy=e, max_episodes=exp_max_episodes, post_batch_fn=None)\n",
    "        \n",
    "    dump_training_data(results, training_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:02.505328Z",
     "iopub.status.busy": "2021-01-26T09:18:02.504831Z",
     "iopub.status.idle": "2021-01-26T09:18:03.109900Z",
     "shell.execute_reply": "2021-01-26T09:18:03.110429Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_experiment_results(results, fig=None):\n",
    "    if fig is None:\n",
    "        fig, _ = plt.subplots(nrows=2, ncols=2, sharex=True, figsize=(18,12))\n",
    "    for i, plot_type in enumerate(('loss_p', 'baseline', 'loss_e', 'mean_reward')):\n",
    "        ax = fig.axes[i]\n",
    "        for exp_name, exp_res in results.items():\n",
    "            if plot_type not in exp_res:\n",
    "                continue\n",
    "            ax.plot(exp_res['episode_num'], exp_res[plot_type], label=exp_name)\n",
    "        ax.set_title(plot_type)\n",
    "        ax.set_xlabel('episode')\n",
    "        ax.legend()\n",
    "    return fig\n",
    "    \n",
    "experiments_results_fig = plot_experiment_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see positive training dynamics in the graphs (reward going up).\n",
    "If you don't, use them to further update your model or hyperparams.\n",
    "\n",
    "To pass the test, you'll need to get a best total mean reward of at least 10 in the fixed number of epochs using the combined loss.\n",
    "It's possible to get much higher (over 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:03.113686Z",
     "iopub.status.busy": "2021-01-26T09:18:03.113197Z",
     "iopub.status.idle": "2021-01-26T09:18:03.137262Z",
     "shell.execute_reply": "2021-01-26T09:18:03.137802Z"
    }
   },
   "outputs": [],
   "source": [
    "best_cpg_mean_reward = max(results['cpg']['mean_reward'])\n",
    "print(f'Best CPG mean reward: {best_cpg_mean_reward:.2f}')\n",
    "\n",
    "test.assertGreater(best_cpg_mean_reward, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at a gameplay video of our `cpg` model after the short training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:03.141412Z",
     "iopub.status.busy": "2021-01-26T09:18:03.140807Z",
     "iopub.status.idle": "2021-01-26T09:18:08.250584Z",
     "shell.execute_reply": "2021-01-26T09:18:08.251102Z"
    }
   },
   "outputs": [],
   "source": [
    "hp = hw4.answers.part1_pg_hyperparams()\n",
    "p_net_cpg = hw4pg.PolicyNet.build_for_env(env, **hp)\n",
    "p_net_cpg.load_state_dict(results['cpg']['model_state'])\n",
    "\n",
    "env, n_steps, reward = hw4pg.PolicyAgent.monitor_episode(ENV_NAME, p_net_cpg)\n",
    "print(f'{n_steps} steps, total reward: {reward:.2f}')\n",
    "show_monitor_video(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic (AAC)\n",
    "<a id=part1_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the policy-gradient loss can be interpreted as a log-likelihood of the policy term\n",
    "(selecting a specific action at a specific state), weighted by the future rewards of that choice of action.\n",
    "\n",
    "However, navely weighting by rewards has significant drawbacks in terms of the variance of the resulting gradient.\n",
    "We addressed this by adding a simple baseline term which represented our \"expected reward\" so that we increase probability of actions\n",
    "leading to trajectories which exceed this expectation and vice-versa.\n",
    "\n",
    "In this part we'll explore a more powerful baseline, which is the idea behind the AAC method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of the state-value function $v_{\\pi}(s)$ and action-value function $q_{\\pi}(s,a)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) &= \\E{g(\\tau)|s_0 = s,\\pi} \\\\\n",
    "q_{\\pi}(s,a) &= \\E{g(\\tau)|s_0 = s,a_0=a,\\pi}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Both these functions represent the value of the state $s$. However, $v_\\pi$ averages over the first action according to the policy,\n",
    "while $q_\\pi$ fixes the first action and then continues according to the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their difference is known as the **advantage function**:\n",
    "$$\n",
    "a_\\pi(s,a) = q_\\pi(s,a)-v_\\pi(s).\n",
    "$$\n",
    "\n",
    "If $a_\\pi(s,a)>0$ it means that it's better (in expectation) to take action $a$ in state $s$ compared to the average action.\n",
    "In other words, $a_\\pi(s,a)$ represents the *advantage* of using action $a$ in state $s$ compared to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used an estimate for $q_\\pi$ as our weighting term for the log-policy, with a fixed baseline per batch.\n",
    "\n",
    "$$\n",
    "\\hat\\grad\\mathcal{L}_{\\text{BPG}}(\\vec{\\theta})\n",
    "=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t\\geq0} \\left(\\hat{q}_{i,t}-b\\right) \\grad\\log \\pi_{\\vec{\\theta}}(a_{i,t}|s_{i,t}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the state value as a baseline, so that an estimate of the advantage function is our weighting term:\n",
    "\n",
    "$$\n",
    "\\hat\\grad\\mathcal{L}_{\\text{AAC}}(\\vec{\\theta})\n",
    "=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t\\geq0} \\left(\\hat{q}_{i,t}-v_\\pi(s_t)\\right) \\grad\\log \\pi_{\\vec{\\theta}}(a_{i,t}|s_{i,t}).\n",
    "$$\n",
    "\n",
    "Intuitively, using the advantage function makes sense because it means we're weighting our policy's actions according to\n",
    "how advantageous they are compared to other possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how will we know $v_\\pi(s)$? We'll learn it of course, using another neural network.\n",
    "This is known as actor-critic learning. We simultaneously learn the policy (actor) and the value of states (critic).\n",
    "We'll treat it as a regression task: given a state $s_t$, our state-value network will output $\\hat{v}_\\pi(s_t)$,\n",
    "an estimate of the actual unknown state-value.\n",
    "Our regression targets will be the discounted rewards, $\\hat{q}_{i,t}$ (see question 2),\n",
    "and we can use a simple MSE as the loss function,\n",
    "$$\n",
    "\\mathcal{L}_{\\text{SV}} = \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t\\geq0}\\left(\\hat{v}_\\pi(s_t) - \\hat{q}_{i,t}\\right)^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build heavily on our implementation of the regular policy-gradient method, and just add a new model class and a new loss class, with a small modification to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the model. It will accept a state, and return action scores (as before), but also the value of that state.\n",
    "You can experiment with a dual-head network that has a shared base, or implement two separate parts within the network.\n",
    "\n",
    "**TODO**:\n",
    "1. Implement the model as the `AACPolicyNet` class in the `hw4/rl_ac.py` module.\n",
    "1. Set the hyperparameters in the `part1_aac_hyperparams()` function of the `hw4.answers` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:08.255291Z",
     "iopub.status.busy": "2021-01-26T09:18:08.254804Z",
     "iopub.status.idle": "2021-01-26T09:18:08.283042Z",
     "shell.execute_reply": "2021-01-26T09:18:08.282479Z"
    }
   },
   "outputs": [],
   "source": [
    "import hw4.rl_ac as hw4ac\n",
    "\n",
    "hp = hw4.answers.part1_aac_hyperparams()\n",
    "pv_net = hw4ac.AACPolicyNet.build_for_env(env, device, **hp)\n",
    "pv_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the agent class, `AACPolicyAgent`, in the `hw4/rl_ac.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:08.286379Z",
     "iopub.status.busy": "2021-01-26T09:18:08.285898Z",
     "iopub.status.idle": "2021-01-26T09:18:08.308713Z",
     "shell.execute_reply": "2021-01-26T09:18:08.309268Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = hw4ac.AACPolicyAgent(env, pv_net, device)\n",
    "exp = agent.step()\n",
    "\n",
    "test.assertIsInstance(exp, hw4pg.Experience)\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the AAC loss function as the class `AACPolicyGradientLoss` in the `hw4/rl_ac.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:08.312928Z",
     "iopub.status.busy": "2021-01-26T09:18:08.312402Z",
     "iopub.status.idle": "2021-01-26T09:18:08.334794Z",
     "shell.execute_reply": "2021-01-26T09:18:08.335304Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn_aac = hw4ac.AACPolicyGradientLoss(delta=1.)\n",
    "test_state_values = torch.ones(test_action_scores.shape[0], 1)\n",
    "loss_t, loss_dict = loss_fn_aac(test_batch, (test_action_scores, test_state_values))\n",
    "\n",
    "print(f'{loss_dict=}')\n",
    "test.assertAlmostEqual(loss_dict['adv_m'], -23.191, delta=1e-2)\n",
    "test.assertAlmostEqual(loss_t.item(), 1183.948, delta=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same experiment as before, but with the AAC method and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:08.338840Z",
     "iopub.status.busy": "2021-01-26T09:18:08.338306Z",
     "iopub.status.idle": "2021-01-26T09:18:08.359046Z",
     "shell.execute_reply": "2021-01-26T09:18:08.359560Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_aac(baseline=False, entropy=False, **train_kwargs):\n",
    "    hp = hw4.answers.part1_aac_hyperparams()\n",
    "    loss_fns = [hw4ac.AACPolicyGradientLoss(hp['delta']), hw4pg.ActionEntropyLoss(ENV_N_ACTIONS, hp['beta'])]\n",
    "    return train_rl(hw4ac.AACPolicyAgent, hw4ac.AACPolicyNet, loss_fns, hp, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:08.363152Z",
     "iopub.status.busy": "2021-01-26T09:18:08.362656Z",
     "iopub.status.idle": "2021-01-26T09:18:08.391276Z",
     "shell.execute_reply": "2021-01-26T09:18:08.391794Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_filename = os.path.join('results', f'part1_exp_aac.dat')\n",
    "\n",
    "# Set to True to force re-run (careful, will delete old experiment results)\n",
    "force_run = False\n",
    "\n",
    "if os.path.isfile(training_data_filename) and not force_run:\n",
    "    print(f'=== results file {training_data_filename} exists, skipping experiments.')\n",
    "    results_aac = load_training_data(training_data_filename)\n",
    "    \n",
    "else:\n",
    "    print(f'=== Running AAC experiment')\n",
    "    training_data = train_aac(max_episodes=exp_max_episodes)\n",
    "    results_aac = dict(aac=training_data)\n",
    "    dump_training_data(results_aac, training_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:08.394569Z",
     "iopub.status.busy": "2021-01-26T09:18:08.394076Z",
     "iopub.status.idle": "2021-01-26T09:18:09.418890Z",
     "shell.execute_reply": "2021-01-26T09:18:09.419393Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments_results_fig = plot_experiment_results(results)\n",
    "plot_experiment_results(results_aac, fig=experiments_results_fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get better results with the AAC method, so this time the bar is higher (again, you should aim for a mean reward of 100+).\n",
    "Compare the graphs with combined PG method and see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:09.422570Z",
     "iopub.status.busy": "2021-01-26T09:18:09.422108Z",
     "iopub.status.idle": "2021-01-26T09:18:09.447846Z",
     "shell.execute_reply": "2021-01-26T09:18:09.447311Z"
    }
   },
   "outputs": [],
   "source": [
    "best_aac_mean_reward = max(results_aac['aac']['mean_reward'])\n",
    "print(f'Best AAC mean reward: {best_aac_mean_reward:.2f}')\n",
    "\n",
    "test.assertGreater(best_aac_mean_reward, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model training and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using your best model and hyperparams, let's train model for much longer and see the performance.\n",
    "Just for fun, we'll also visualize an episode every now and then so that we can see how well the agent is playing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "- Run the following block to train.\n",
    "- Tweak model or hyperparams as necessary.\n",
    "- Aim for high mean reward, at least 150+. It's possible to get over 200.\n",
    "- When training is done and you're satisfied with the model's outputs, rename the checkpoint file by adding `_final` to the file name.\n",
    "  This will cause the block to skip training and instead load your saved model when running the homework submission script.\n",
    "  Note that your submission zip file will not include the checkpoint file. This is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:09.455548Z",
     "iopub.status.busy": "2021-01-26T09:18:09.455018Z",
     "iopub.status.idle": "2021-01-26T09:18:13.342220Z",
     "shell.execute_reply": "2021-01-26T09:18:13.342741Z"
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "CHECKPOINTS_FILE = f'checkpoints/{ENV_NAME}-ac.dat'\n",
    "CHECKPOINTS_FILE_FINAL = f'checkpoints/{ENV_NAME}-ac_final.dat'\n",
    "TARGET_REWARD = 125\n",
    "MAX_EPISODES = 15_000\n",
    "\n",
    "def post_batch_fn(batch_idx, p_net, batch, print_every=20, final=False):\n",
    "    if not final and batch_idx % print_every != 0:\n",
    "        return\n",
    "    env, n_steps, reward = hw4ac.AACPolicyAgent.monitor_episode(ENV_NAME, p_net)\n",
    "    html = show_monitor_video(env, width=\"500\")\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(f'Monitor@#{batch_idx}: n_steps={n_steps}, total_reward={reward:.3f}, final={final}')\n",
    "    IPython.display.display_html(html)\n",
    "    \n",
    "    \n",
    "if os.path.isfile(CHECKPOINTS_FILE_FINAL):\n",
    "    print(f'=== {CHECKPOINTS_FILE_FINAL} exists, skipping training...')\n",
    "    checkpoint_data = torch.load(CHECKPOINTS_FILE_FINAL)\n",
    "    hp = hw4.answers.part1_aac_hyperparams()\n",
    "    pv_net = hw4ac.AACPolicyNet.build_for_env(env, **hp)\n",
    "    pv_net.load_state_dict(checkpoint_data['params'])\n",
    "    print(f'=== Running best model...')\n",
    "    env, n_steps, reward = hw4ac.AACPolicyAgent.monitor_episode(ENV_NAME, pv_net)\n",
    "    print(f'=== Best model ran for {n_steps} steps. Total reward: {reward:.2f}')\n",
    "    IPython.display.display_html(show_monitor_video(env))\n",
    "    best_mean_reward = checkpoint_data[\"best_mean_reward\"]\n",
    "else:\n",
    "    print(f'=== Starting training...')\n",
    "    train_data = train_aac(TARGET_REWARD, max_episodes=MAX_EPISODES,\n",
    "                           seed=None, checkpoints_file=CHECKPOINTS_FILE, post_batch_fn=post_batch_fn)\n",
    "    print(f'=== Done, ', end='')\n",
    "    best_mean_reward = train_data[\"best_mean_reward\"][-1]\n",
    "    print(f'num_episodes={train_data[\"episode_num\"][-1]}, best_mean_reward={best_mean_reward:.1f}')\n",
    "          \n",
    "test.assertGreaterEqual(best_mean_reward, TARGET_REWARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "<a id=part1_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Answer the following questions. Write your answers in the appropriate variables in the module `hw4/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:13.346023Z",
     "iopub.status.busy": "2021-01-26T09:18:13.345546Z",
     "iopub.status.idle": "2021-01-26T09:18:13.371344Z",
     "shell.execute_reply": "2021-01-26T09:18:13.371698Z"
    }
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw4.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Explain qualitatively why subtracting a baseline in the policy-gradient helps reduce it's variance.\n",
    "Specifically, give an example where it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:13.374868Z",
     "iopub.status.busy": "2021-01-26T09:18:13.374386Z",
     "iopub.status.idle": "2021-01-26T09:18:13.396776Z",
     "shell.execute_reply": "2021-01-26T09:18:13.397289Z"
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw4.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "In AAC, when using the estimated q-values as regression targets for our state-values, why do we get a valid approximation?\n",
    "Hint: how is $v_\\pi(s)$ expressed in terms of $q_\\pi(s,a)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:13.400191Z",
     "iopub.status.busy": "2021-01-26T09:18:13.399648Z",
     "iopub.status.idle": "2021-01-26T09:18:13.421561Z",
     "shell.execute_reply": "2021-01-26T09:18:13.422116Z"
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw4.answers.part1_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "1. Analyze and explain the graphs you got in first experiment run.\n",
    "2. Compare the experiment graphs you got with the AAC method to the regular PG method (`cpg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:13.425046Z",
     "iopub.status.busy": "2021-01-26T09:18:13.424576Z",
     "iopub.status.idle": "2021-01-26T09:18:13.446311Z",
     "shell.execute_reply": "2021-01-26T09:18:13.446861Z"
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw4.answers.part1_q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
